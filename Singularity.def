Bootstrap: docker
From: pytorch/pytorch:2.4.1-cuda12.1-cudnn9-runtime

%environment
    # --- Runtime behaviour ---
    export TRANSFORMERS_OFFLINE=1
    export HF_HUB_OFFLINE=1
    export WANDB_DISABLED=true
    export TOKENIZERS_PARALLELISM=false

    # --- HF caches (werden im Slurm auf $SLURM_TMPDIR gemappt) ---
    export HF_HOME=/tmp/.hf_cache
    export TRANSFORMERS_CACHE=${HF_HOME}/transformers
    export HF_DATASETS_CACHE=${HF_HOME}/datasets
    export HUGGINGFACE_HUB_CACHE=${HF_HOME}/hub

%files
    requirements.txt /opt/requirements.txt
    download_models_into_container.py /opt/download_models_into_container.py

%post
    set -eux

    # ------------------------------------------------------------
    # System basics
    # ------------------------------------------------------------
    apt-get update
    apt-get install -y --no-install-recommends \
        ca-certificates \
        git
    rm -rf /var/lib/apt/lists/*

    # ------------------------------------------------------------
    # Python tooling
    # ------------------------------------------------------------
    python -m pip install --upgrade pip setuptools wheel

    # ------------------------------------------------------------
    # Core ML / HF stack (PINNED + force reinstall!)
    # This guarantees no transformers 5.0.0.dev0 leaks in
    # ------------------------------------------------------------
    python -m pip install --upgrade --force-reinstall \
        certifi \
        requests \
        urllib3 \
        idna \
        charset_normalizer \
        huggingface_hub \
        transformers==4.40.2 \
        tokenizers==0.15.2 \
        datasets==2.21.0 \
        accelerate==0.33.0 \
        sentencepiece \
        protobuf

    # ------------------------------------------------------------
    # Repo requirements
    # ------------------------------------------------------------
    python -m pip install -r /opt/requirements.txt

    # ------------------------------------------------------------
    # Download models INTO the container (online at build time)
    # ------------------------------------------------------------
    python /opt/download_models_into_container.py

    # ------------------------------------------------------------
    # Smoke test (fails build if something is wrong)
    # ------------------------------------------------------------
    python - <<'PY'
import sys, torch, transformers, certifi, requests, huggingface_hub
print("python:", sys.version)
print("torch:", torch.__version__)
print("transformers:", transformers.__version__)
print("certifi:", certifi.__version__)
print("requests:", requests.__version__)
print("huggingface_hub:", huggingface_hub.__version__)
PY

%runscript
    exec python "$@"
